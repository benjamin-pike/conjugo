{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from googletrans import Translator\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output as clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('').replace('scripts', '')\n",
    "data_dir = os.path.join(path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e532160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nestedDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        value = self[key] = type(self)()\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---HEADERS FOR WEB SCRAPING---#\n",
    "headers = {\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "           \"accept-encoding\": \"gzip, deflate, br\",\n",
    "           \"accept-language\": \"en-GB,en;q=0.9,es-ES;q=0.8,es;q=0.7,en-US;q=0.6,eu;q=0.5\",\n",
    "           \"cache-control\": \"max-age=0\",\n",
    "           \"cookie\": \"t=238707487; _ga=GA1.2.1376835774.1641262578; _gid=GA1.2.1482423077.1641262578; _fbp=fb.1.1641262579526.851471446\",\n",
    "           \"referer\": \"https://hidemy.name/en/proxy-list/?start=64\",\n",
    "           \"sec-ch-ua-mobile\": \"?0\",\n",
    "           \"sec-ch-ua-platform\": \"macOS\",\n",
    "           \"sec-fetch-dest\": \"document\",\n",
    "           \"sec-fetch-mode\": \"navigate\",\n",
    "           \"sec-fetch-site\": \"same-origin\",\n",
    "           \"sec-fetch-user\": \"?1\",\n",
    "           \"upgrade-insecure-requests\": \"1\",\n",
    "           \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAdditionalVerbs(verb):\n",
    "\n",
    "    url = f'https://api.verbix.com/conjugator/iv1/ab8e7bb5-9ac6-11e7-ab6a-00089be4dcbc/1/13/113/{verb}'\n",
    "    page = requests.get(url, headers = headers)\n",
    "    \n",
    "    try: \n",
    "        similar_raw = re.search(r'Verbs conjugated like(.*?)<h3>', page.text).group(1)\n",
    "        similar_parsed = re.findall(r'\\\\\">(.*?)</a>', similar_raw)\n",
    "        similar_verbs = [x for x in similar_parsed if x.isalpha()]\n",
    "    except: similar_verbs = []\n",
    "\n",
    "    try: \n",
    "        prefix_raw = re.search(r'Other Verbs with Separable Prefix(.*?)<h3>', page.text).group(1)\n",
    "        prefix_parsed = re.findall(r'\\\\\">(.*?)</a>', prefix_raw)\n",
    "        prefix_verbs = [x for x in prefix_parsed if x.isalpha()]\n",
    "    except: prefix_verbs = []\n",
    "\n",
    "    try: \n",
    "        base_raw = re.search(r'Other Verbs with the same Base Verb(.*?)<h3>', page.text).group(1)\n",
    "        base_parsed = re.findall(r'\\\\\">(.*?)</a>', base_raw)\n",
    "        base_verbs = [x for x in base_parsed if x.isalpha()]\n",
    "    except: base_verbs = []\n",
    "\n",
    "    try: \n",
    "        synonyms_raw = re.search(r'<h4>Synonyms</h4>(.*?)<h3>', page.text).group(1)\n",
    "        synonyms_parsed = re.findall(r'\\\\\">(.*?)</a>', synonyms_raw)\n",
    "        synonyms_verbs = [x for x in synonyms_parsed if x.isalpha()]\n",
    "    except: synonyms_verbs = []\n",
    "\n",
    "    return list(set(similar_verbs + prefix_verbs + base_verbs + synonyms_verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgramData(verb, language, years):\n",
    "    corpus = {'spanish':32, 'french':19, 'italian':22, 'german':20}\n",
    "    syear,eyear = years\n",
    "    raw = requests.get(f'https://books.google.com/ngrams/json?content={verb}&year_start={syear}&year_end={eyear}&corpus={corpus[language]}&smoothing=0', headers = headers)\n",
    "        \n",
    "    if raw.text != '[]': data = json.loads(raw.text)[0]\n",
    "    else: return 'ngram not found'\n",
    "    \n",
    "    if data['ngram'] == verb:\n",
    "        values = data['timeseries']\n",
    "        return sum(values)/len(values)\n",
    "    else: return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRegularity(verb, page):\n",
    "\n",
    "    if 'NOTRECOGVERB' in page.text:\n",
    "        return 'x'\n",
    "    elif \"class=\\\\\\\"irregular\\\\\\\">\" in page.text:\n",
    "        return 'i'\n",
    "    elif \"class=\\\\\\\"orto\\\\\\\">\" in page.text:\n",
    "        return 'sc'\n",
    "    else:\n",
    "        return 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatResults(string):\n",
    "    ich = re.search(r'\\\\nich(.*?)\\\\', string.text).group(1)\n",
    "    du = re.search(r'\\\\ndu(.*?)\\\\', string.text).group(1)\n",
    "    es = re.search(r'\\\\ner;sie;es(.*?)\\\\', string.text).group(1)\n",
    "    wir = re.search(r'\\\\nwir(.*?)\\\\', string.text).group(1)\n",
    "    ihr = re.search(r'\\\\nihr(.*?)\\\\', string.text).group(1)\n",
    "    Sie = re.search(r'\\\\nsie;Sie(.*?)\\\\', string.text).group(1)\n",
    "\n",
    "    ich = re.sub('[\\(].*?[\\)]', '', ich).split(\";\")[0]\n",
    "    du = re.sub('[\\(].*?[\\)]', '', du).split(\";\")[0]\n",
    "    es = re.sub('[\\(].*?[\\)]', '', es).split(\";\")[0]\n",
    "    wir = re.sub('[\\(].*?[\\)]', '', wir).split(\";\")[0]\n",
    "    ihr = re.sub('[\\(].*?[\\)]', '', ihr).split(\";\")[0]\n",
    "    Sie = re.sub('[\\(].*?[\\)]', '', Sie).split(\";\")[0]\n",
    "\n",
    "    return [ich, du, es, wir, ihr, Sie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formSubjunctiveFuture(verb, past_participle = ''):\n",
    "    werden = ['werde', 'werdest', 'werde', 'werden', 'werdet', 'werden']\n",
    "    if past_participle:\n",
    "        return [f\"{w} {verb} {past_participle}\" for w in werden]\n",
    "    else:\n",
    "        return [f\"{w} {verb}\" for w in werden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatImperative(string, n = ''):\n",
    "    du = re.search(r'\\\\ndu(.*?)\\\\', string.text).group(1)\n",
    "    ihr = re.search(r'\\\\nihr(.*?)\\\\', string.text).group(1)\n",
    "    wir = ihr[:-1] + 'en wir'\n",
    "    Sie = ihr[:-1] + 'en Sie'\n",
    "\n",
    "    du = re.sub('[\\(].*?[\\)]', '', du).split(\";\")[0]\n",
    "    wir = re.sub('[\\(].*?[\\)]', '', wir).split(\";\")[0]\n",
    "    ihr = re.sub('[\\(].*?[\\)]', '', ihr).split(\";\")[0]\n",
    "    Sie = re.sub('[\\(].*?[\\)]', '', Sie).split(\";\")[0]\n",
    "\n",
    "    if n:\n",
    "        return [du + ' nicht', wir + ' nicht', ihr + ' nicht', Sie + ' nicht']\n",
    "    else:\n",
    "        return [du, wir, ihr, Sie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SCRAPE REVERSO FOR TRANSLATIONS---#\n",
    "def getReversoTranslations(verb):\n",
    "\n",
    "    translations = []\n",
    "\n",
    "    page = requests.get(f'https://context.reverso.net/translation/german-english/{verb}', headers = headers)\n",
    "    soup = bs4.BeautifulSoup(page.text)\n",
    "\n",
    "    if verb in soup.select('title')[0].getText():\n",
    "\n",
    "        while len(soup.find_all('div', {\"class\": \"mobile-hidden\"})) > 0:\n",
    "            soup.find_all('div', {\"class\": \"mobile-hidden\"})[0].extract()\n",
    "        \n",
    "        while len(soup.find_all('a', {\"class\": \"mobile-hidden\"})) > 0:  \n",
    "            soup.find_all('a', {\"class\": \"mobile-hidden\"})[0].extract()\n",
    "\n",
    "        for i in range(0, len(soup.select('#translations-content .translation.ltr.dict.v'))):\n",
    "            translations.append(soup.select('#translations-content .translation.ltr.dict.v')[i].getText().replace('\\n\\n\\n\\r\\n          ','').replace('\\n',''))\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---USE GOOGLETRANS PACKAGE TO FIND TRANSLATONS---#\n",
    "def getGoogleTranslations(verb):\n",
    "    \n",
    "    try: \n",
    "        translations = translator.translate(verb, dest = 'en', src = 'de').extra_data['all-translations']\n",
    "\n",
    "        if translations != None:\n",
    "            for x in range(0, len(translations)):\n",
    "                if translations[x][0] == 'verb':\n",
    "                    translations = translations[x][1]\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            translations = []\n",
    "                    \n",
    "    except Exception as ex:\n",
    "            translations = []\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate(verb_data):\n",
    "    \n",
    "    verb,rank = verb_data\n",
    "\n",
    "    url = f'https://api.verbix.com/conjugator/iv1/ab8e7bb5-9ac6-11e7-ab6a-00089be4dcbc/1/13/113/{verb}'\n",
    "    page = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(page.text, 'lxml') \n",
    "\n",
    "    if rank < 50000:\n",
    "\n",
    "        dictionary = nestedDict()\n",
    "\n",
    "        dictionary['infinitive'] = verb\n",
    "        dictionary['rank'] = rank\n",
    "        dictionary['regularity'] = checkRegularity(verb, page)\n",
    "\n",
    "        dictionary['translations'] = []\n",
    "\n",
    "        dictionary['participle'] = {'present': re.search(r'Present participle: (.*?)\\\\', soup.select('body')[0].getText()).group(1),\n",
    "                                    'past': re.search(r'Past participle: (.*?)\\\\', soup.select('body')[0].getText()).group(1)}\n",
    "\n",
    "        subjects_indicies = [0, 1, 2, 2, 2, 3, 4, 5]\n",
    "        subjects_pronouns = ['ich', 'du', 'er', 'sie', 'es', 'wir', 'ihr', 'Sie']\n",
    "\n",
    "        subjects_indicies_imp = [0, 1, 2, 3]\n",
    "        subjects_pronouns_imp = ['du', 'wir', 'ihr', 'Sie']\n",
    "\n",
    "        for i,p in zip(subjects_indicies, subjects_pronouns):\n",
    "\n",
    "            #indicative\n",
    "            dictionary['simple']['indicative']['present'][p] = formatResults(soup.select('table')[0])[i]\n",
    "            dictionary['simple']['indicative']['imperfect'][p] = formatResults(soup.select('table')[2])[i]\n",
    "            dictionary['simple']['indicative']['future'][p] = formatResults(soup.select('table')[4])[i]\n",
    "\n",
    "            #subjunctive\n",
    "            dictionary['simple']['subjunctive']['present'][p] = formatResults(soup.select('table')[6])[i]\n",
    "            dictionary['simple']['subjunctive']['imperfect'][p] = formatResults(soup.select('table')[8])[i]\n",
    "            dictionary['simple']['subjunctive']['future'][p] = formSubjunctiveFuture(verb)[i]\n",
    "            dictionary['simple']['subjunctive']['conditional'][p] = formatResults(soup.select('table')[10])[i]\n",
    "\n",
    "            #perfect indicative\n",
    "            dictionary['compound']['indicative']['present'][p] = formatResults(soup.select('table')[1])[i]\n",
    "            dictionary['compound']['indicative']['imperfect'][p] = formatResults(soup.select('table')[3])[i]\n",
    "            dictionary['compound']['indicative']['future'][p] = formatResults(soup.select('table')[5])[i]\n",
    "\n",
    "            #subjunctive\n",
    "            dictionary['compound']['subjunctive']['present'][p] = formatResults(soup.select('table')[7])[i]\n",
    "            dictionary['compound']['subjunctive']['imperfect'][p] = formatResults(soup.select('table')[9])[i]\n",
    "            dictionary['compound']['subjunctive']['future'][p] = formSubjunctiveFuture(verb, dictionary['participle']['past'])[i]\n",
    "            dictionary['compound']['subjunctive']['conditional'][p] = formatResults(soup.select('table')[11])[i]\n",
    "\n",
    "\n",
    "            #imperative\n",
    "            try:\n",
    "                for i,p in zip(subjects_indicies_imp, subjects_pronouns_imp):\n",
    "                    dictionary['simple']['imperative']['affirmative'][p] = formatImperative(soup.select('table')[12])[i]\n",
    "                    dictionary['simple']['imperative']['negative'][p] = formatImperative(soup.select('table')[12], 'negative')[i]\n",
    "            except:\n",
    "                for i,p in zip(subjects_indicies_imp, subjects_pronouns_imp):\n",
    "                    dictionary['simple']['imperative']['affirmative'][p] = ''\n",
    "                    dictionary['simple']['imperative']['negative'][p] = ''\n",
    "    \n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SCRAPE UNRANKED VERBS FROM COOLJUGATOR---#\n",
    "page = requests.get('https://cooljugator.com/de/list/all', headers = headers)\n",
    "soup = bs4.BeautifulSoup(page.text, 'lxml')\n",
    "coolverbs = []\n",
    "\n",
    "for item in soup.select('.ui.segment.stacked .item'):\n",
    "    verb = item.getText().split(' ')[0]\n",
    "    if len(item.getText().split(' ')) == 3 and verb.isalpha() and verb[-1] == 'n':\n",
    "        coolverbs.append(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SCRAPE UNRANKED VERBS FROM WIKIPEDIA---#\n",
    "wikiverbs = []\n",
    "urlsafe = set([x for x in 'abcdefghijklmnopqrstuvwxyz'])\n",
    "last = 'aalen'\n",
    "run = True\n",
    "\n",
    "while run:\n",
    "\n",
    "    url = f'https://en.wiktionary.org/w/index.php?title=Category:German_verbs&pagefrom={last}'\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = bs4.BeautifulSoup(page.text, 'lxml') \n",
    "    list_items = soup.select('.mw-content-ltr .mw-category li a')\n",
    "\n",
    "    for i in range(18, len(list_items)):\n",
    "        verb = list_items[i].getText()\n",
    "\n",
    "        if verb not in wikiverbs and ' ' not in verb and verb.isalpha() and verb[-1] == 'n':\n",
    "            wikiverbs.append(verb.lower())\n",
    "            clear(wait = True); print(wikiverbs[-1])\n",
    "    \n",
    "    for verb in reversed(wikiverbs):\n",
    "        split = set([x for x in verb])  \n",
    "        if len(split - urlsafe) == 0:\n",
    "            if verb != last:\n",
    "                last = verb\n",
    "                break\n",
    "            else:\n",
    "                run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---COMBINE VERBS FROM COOLJUGATOR AND WIKIPEDIA---#\n",
    "all_verbs = list(set(coolverbs + wikiverbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---INITIALISE VARIABLES FOR STORING ADDITIONAL VERBS---#\n",
    "total = all_verbs\n",
    "new_verbs = []\n",
    "initlen = len(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET INDEX AT WHICH TO FINDING ADDITIONAL VERBS---#\n",
    "start = 6663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FIND ADDITIONAL VERBS USING VERBIX---#\n",
    "for i in range(start, len(all_verbs)):\n",
    "    \n",
    "    additional = findAdditionalVerbs(all_verbs[i])\n",
    "    \n",
    "    for verb in additional:\n",
    "        if verb not in total:\n",
    "            total.append(verb)\n",
    "            new_verbs.append(verb)\n",
    "    \n",
    "    clear(wait = True); print(f\"{all_verbs[i]} – {initlen} -> {len(total)} | {len(new_verbs)} ({round(i*100/initlen, 3)}%)\")\n",
    "\n",
    "all_verbs = total; del total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SAVE ADDITIONAL VERBS---#\n",
    "f = os.path.join(data_dir, 'infinitives_german_intermediate_additional.json')\n",
    "\n",
    "with open(f, 'w', encoding = 'utf8') as file:\n",
    "    json.dump(all_verbs, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–––OPEN CHECKPOINT FILE---#\n",
    "f = os.path.join(data_dir, 'infinitives_german_intermediate_additional.json')\n",
    "\n",
    "with open(f, 'r', encoding = 'utf8') as file:\n",
    "    all_verbs = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET VARIABLES FOR NGRAM SCORES---#\n",
    "verbs = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–––OPEN CHECKPOINT FILE---#\n",
    "f = os.path.join(data_dir, 'infinitives_german_intermediate.json')\n",
    "\n",
    "with open(f, 'r', encoding = 'utf8') as file:\n",
    "    verbs = json.loads(file.read())\n",
    "\n",
    "found = [x[0] for x in verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emporklettern', 1.5519067125397285e-08] (99.992%)\n"
     ]
    }
   ],
   "source": [
    "#---ITERATE THROUGH ALL VERBS AND FIND NGRAM SCORES---#\n",
    "while i < len(all_verbs):\n",
    "    try:\n",
    "        verb = all_verbs[i]\n",
    "\n",
    "        if verb not in found:\n",
    "            value = getNgramData(verb, 'german', (1980, 2005))\n",
    "\n",
    "            if isinstance(value, float):\n",
    "                verbs.append([verb, value])\n",
    "                clear(wait = True); print(f\"{verbs[-1]} ({round(i*100/len(all_verbs), 3)}%)\")\n",
    "\n",
    "            if i % 250 == 0:\n",
    "                with open(\"../data/infinitives_german_intermediate.json\", 'w', encoding = 'utf8') as file:\n",
    "                    json.dump(verbs, file, indent = 4, ensure_ascii = False)\n",
    "\n",
    "            time.sleep(1)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    except Exception as ex:\n",
    "        clear(wait = True); print(f\"error ({ex}) – sleeping for 2 minutes ({round(i*100/len(all_verbs), 3)}%)\")\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---RANK VERBS ACCORDING TO NGRAM SCORES---#\n",
    "infinitives = []\n",
    "rank = 1\n",
    "\n",
    "for verb in sorted(verbs, key = lambda x: x[1])[::-1]:\n",
    "    infinitives.append([verb[0], rank])\n",
    "    rank += 1\n",
    "\n",
    "verbs = infinitives; del infinitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11879 -> 11695\n"
     ]
    }
   ],
   "source": [
    "#---FILTER DUPLICATES AND RERANK VERBS---#\n",
    "decoded = []\n",
    "unique = []\n",
    "rank = 1\n",
    "\n",
    "for verb in verbs:\n",
    "    if unidecode(verb[0]) not in decoded:\n",
    "        decoded.append(unidecode(verb[0]))\n",
    "        unique.append([verb[0], rank])\n",
    "        rank += 1\n",
    "\n",
    "print(f\"{len(verbs)} -> {len(unique)}\")\n",
    "\n",
    "infinitives = unique; del unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CREATE INTERMEDIATE CHECKPOINT FILE AS CONTINGENCY IN CASE OF KERNAL TIMEOUT---#\n",
    "f = os.path.join(data_dir, 'infinitives_german_intermediate.json')\n",
    "\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(infinitives, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---OPEN CHECKPOINT FILE---#\n",
    "f = os.path.join(data_dir, 'infinitives_german_intermediate.json')\n",
    "\n",
    "with open(f, \"r\", encoding = 'utf8') as file:\n",
    "    infinitives = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET VARIABLES FOR CONJUGATIONS---#\n",
    "conjugations = defaultdict()\n",
    "defective = set()\n",
    "skipped = []\n",
    "rank = 1\n",
    "exclude = ['einen', 'deutschen', 'alten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET INDEX AT WHICH TO START CONJUGATIONS---#\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramifizieren – complete (99.991%)\n"
     ]
    }
   ],
   "source": [
    "#---GENERATE CONJUGATIONS---#\n",
    "for i in range(start, len(infinitives)):\n",
    "\n",
    "    infinitive = infinitives[i]\n",
    "    try:\n",
    "        c = conjugate(infinitive)\n",
    "\n",
    "        if c != None and infinitive[0] not in exclude: \n",
    "            conjugations[c['infinitive']] = c\n",
    "            conjugations[infinitive[0]]['rank'] = rank\n",
    "            rank += 1\n",
    "\n",
    "            clear(wait = True), print(f\"{infinitive[0]} – complete ({round(i*100/len(infinitives), 3)}%)\")\n",
    "        \n",
    "        else:\n",
    "            clear(wait = True), print(f\"{infinitive[0]} – skipped ({round(i*100/len(infinitives), 3)}%)\")\n",
    "            skipped.append(infinitive)\n",
    "\n",
    "    except Exception as ex:\n",
    "        clear(wait = True), print(f\"{infinitive[0]} – skipped – {ex} ({round(i*100/len(infinitives), 3)}%)\")\n",
    "        skipped.append(infinitive)\n",
    "\n",
    "conjugations = dict(conjugations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8945 -> 8945\n"
     ]
    }
   ],
   "source": [
    "#---REMOVE VERBS WITH UNKNOWN OR CONFLICTING REGULARITIES---#\n",
    "remove = []\n",
    "rank = 1\n",
    "\n",
    "for verb in conjugations:\n",
    "    if conjugations[verb]['regularity'] == 'x':\n",
    "        remove.append(verb)\n",
    "    else:\n",
    "        conjugations[verb]['rank'] = rank\n",
    "        rank += 1\n",
    "\n",
    "print(f\"{len(conjugations)} -> {len(conjugations) - len(remove)}\")\n",
    "\n",
    "for verb in remove:\n",
    "    conjugations.pop[verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CREATE INTERMEDIATE CHECKPOINT FILE AS CONTINGENCY IN CASE OF KERNAL TIMEOUT---#\n",
    "with open(os.path.join(data_dir, 'conjugations_german_intermediate.json'), \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(conjugations, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET INDEX VARIABLE TO 1 BY DEFAULT---#\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---OPEN CHECKPOINT FILE---#\n",
    "with open(os.path.join(data_dir, 'conjugations_translated_german_intermediate.json'), \"r\", encoding = 'utf8') as file:\n",
    "    conjugations = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramifizieren (8945) – complete (100.0%)\n"
     ]
    }
   ],
   "source": [
    "#---ADD TRANSLATIONS---#\n",
    "for verb in conjugations:\n",
    "\n",
    "    if conjugations[verb]['rank'] > i - 1 and conjugations[verb]['translations'] == []:\n",
    "        \n",
    "        translations = getReversoTranslations(verb)\n",
    "\n",
    "        if not translations:\n",
    "            translations = getGoogleTranslations(verb)\n",
    "            time.sleep(9)\n",
    "\n",
    "        conjugations[verb]['translations'] = translations\n",
    "\n",
    "        if conjugations[verb]['rank'] % 200 == 0:\n",
    "            with open(os.path.join(data_dir, 'conjugations_translated_german_intermediate.json'), \"w\", encoding = 'utf8') as file:\n",
    "                json.dump(conjugations, file, indent = 4, ensure_ascii = False)\n",
    "\n",
    "        clear(wait = True), print(f\"{verb} ({i}) – complete ({round(i*100/len(conjugations), 3)}%)\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---REMOVE VERBS WITH NO TRANSLATIONS---#\n",
    "remove = []\n",
    "rank = 1\n",
    "\n",
    "for verb in conjugations:\n",
    "    if conjugations[verb]['translations'] == []:\n",
    "        remove.append(verb)\n",
    "    else:\n",
    "        conjugations[verb]['rank'] = rank\n",
    "        rank += 1\n",
    "\n",
    "for verb in remove:\n",
    "    conjugations.pop(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SAVE TO JSON---#\n",
    "f = os.path.join(data_dir, 'conjugations_german.json')\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(conjugations, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---PARSE INFINITIVES AND SAVE TO JSON---#\n",
    "infinitives = []\n",
    "\n",
    "for verb in conjugations:\n",
    "    infinitives.append([verb, conjugations[verb]['rank'], conjugations[verb]['regularity']])\n",
    "\n",
    "f = os.path.join(data_dir, 'infinitives_german.json')\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(infinitives, file, indent = 4, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9596ba951ddb21d01b6d37648fb60066e40c7f4a0a12dddda8d43c6adacb1362"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
