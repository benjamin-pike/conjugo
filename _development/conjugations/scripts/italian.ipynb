{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import copy\n",
    "from googletrans import Translator\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output as clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('').replace('scripts', '')\n",
    "data_dir = os.path.join(path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nestedDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        value = self[key] = type(self)()\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---HEADERS FOR WEB SCRAPING---#\n",
    "headers = {\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "           \"accept-encoding\": \"gzip, deflate, br\",\n",
    "           \"accept-language\": \"en-GB,en;q=0.9,es-ES;q=0.8,es;q=0.7,en-US;q=0.6,eu;q=0.5\",\n",
    "           \"cache-control\": \"max-age=0\",\n",
    "           \"cookie\": \"t=238707487; _ga=GA1.2.1376835774.1641262578; _gid=GA1.2.1482423077.1641262578; _fbp=fb.1.1641262579526.851471446\",\n",
    "           \"referer\": \"https://hidemy.name/en/proxy-list/?start=64\",\n",
    "           \"sec-ch-ua-mobile\": \"?0\",\n",
    "           \"sec-ch-ua-platform\": \"macOS\",\n",
    "           \"sec-fetch-dest\": \"document\",\n",
    "           \"sec-fetch-mode\": \"navigate\",\n",
    "           \"sec-fetch-site\": \"same-origin\",\n",
    "           \"sec-fetch-user\": \"?1\",\n",
    "           \"upgrade-insecure-requests\": \"1\",\n",
    "           \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAdditionalVerbs(verb):\n",
    "\n",
    "    url = f'https://api.verbix.com/conjugator/iv1/ab8e7bb5-9ac6-11e7-ab6a-00089be4dcbc/1/4/104/{verb}'\n",
    "    page = requests.get(url, headers = headers)\n",
    "    \n",
    "    try: \n",
    "        similar_raw = re.search(r'Verbs conjugated like(.*?)<h3>', page.text).group(1)\n",
    "        similar_parsed = re.findall(r'\\\\\">(.*?)</a>', similar_raw)\n",
    "        similar_verbs = [x for x in similar_parsed if x.isalpha()]\n",
    "    except: similar_verbs = []\n",
    "\n",
    "    try: \n",
    "        prefix_raw = re.search(r'Other Verbs with Separable Prefix(.*?)<h3>', page.text).group(1)\n",
    "        prefix_parsed = re.findall(r'\\\\\">(.*?)</a>', prefix_raw)\n",
    "        prefix_verbs = [x for x in prefix_parsed if x.isalpha()]\n",
    "    except: prefix_verbs = []\n",
    "\n",
    "    try: \n",
    "        base_raw = re.search(r'Other Verbs with the same Base Verb(.*?)<h3>', page.text).group(1)\n",
    "        base_parsed = re.findall(r'\\\\\">(.*?)</a>', base_raw)\n",
    "        base_verbs = [x for x in base_parsed if x.isalpha()]\n",
    "    except: base_verbs = []\n",
    "\n",
    "    try: \n",
    "        synonyms_raw = re.search(r'<h4>Synonyms</h4>(.*?)<h3>', page.text).group(1)\n",
    "        synonyms_parsed = re.findall(r'\\\\\">(.*?)</a>', synonyms_raw)\n",
    "        synonyms_verbs = [x for x in synonyms_parsed if x.isalpha()]\n",
    "    except: synonyms_verbs = []\n",
    "\n",
    "    return list(set(similar_verbs + prefix_verbs + base_verbs + synonyms_verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgramData(verb, language, years):\n",
    "    corpus = {'spanish':32, 'french':19, 'italian':22, 'german':20}\n",
    "    syear,eyear = years\n",
    "    raw = requests.get(f'https://books.google.com/ngrams/json?content={verb}&year_start={syear}&year_end={eyear}&corpus={corpus[language]}&smoothing=0', headers = headers)\n",
    "        \n",
    "    if raw.text != '[]': data = json.loads(raw.text)[0]\n",
    "    else: return 'ngram not found'\n",
    "    \n",
    "    if data['ngram'] == verb:\n",
    "        values = data['timeseries']\n",
    "        return sum(values)/len(values)\n",
    "    else: return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRegularity(verb, page):\n",
    "\n",
    "    if 'NOTRECOGVERB' in page.text:\n",
    "        return 'x'\n",
    "    elif \"class=\\\\\\\"irregular\\\\\\\">\" in page.text:\n",
    "        return 'i'\n",
    "    elif \"class=\\\\\\\"orto\\\\\\\">\" in page.text:\n",
    "        return 'sc'\n",
    "    else:\n",
    "        return 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0f807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatResults(string):\n",
    "    io = re.search(r'\\\\nio(.*?)\\\\', string.text).group(1)\n",
    "    tu = re.search(r'\\\\ntu(.*?)\\\\', string.text).group(1)\n",
    "    lui = re.search(r'\\\\nlui(.*?)\\\\', string.text).group(1)\n",
    "    noi = re.search(r'\\\\nnoi(.*?)\\\\', string.text).group(1)\n",
    "    voi = re.search(r'\\\\nvoi(.*?)\\\\', string.text).group(1)\n",
    "    loro = re.search(r'\\\\nloro(.*?)\\\\', string.text).group(1)\n",
    "\n",
    "    io = re.sub('[\\(].*?[\\)]', '', io).split(\";\")[0]\n",
    "    tu = re.sub('[\\(].*?[\\)]', '', tu).split(\";\")[0]\n",
    "    lui = re.sub('[\\(].*?[\\)]', '', lui).split(\";\")[0]\n",
    "    noi = re.sub('[\\(].*?[\\)]', '', noi).split(\";\")[0]\n",
    "    voi = re.sub('[\\(].*?[\\)]', '', voi).split(\";\")[0]\n",
    "    loro = re.sub('[\\(].*?[\\)]', '', loro).split(\";\")[0]\n",
    "\n",
    "    collated = [io, tu, lui, noi, voi, loro]\n",
    "\n",
    "    for i in range(0, len(collated)):\n",
    "        if \";\" in collated[i]:\n",
    "            if len(collated[i].split(\" \")) == 4:\n",
    "                collated[i] = collated[i].split(\"; \")[1]\n",
    "            else:\n",
    "                collated[i] = collated[i].split(\"; \")[0]\n",
    "\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f27938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStare():\n",
    "    url = f'https://api.verbix.com/conjugator/iv1/ab8e7bb5-9ac6-11e7-ab6a-00089be4dcbc/1/4/104/stare'\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = bs4.BeautifulSoup(page.text, 'lxml') \n",
    "\n",
    "    dictionary = nestedDict()\n",
    "\n",
    "    subjects_indicies = [0, 1, 2, 2, 3, 4, 5]\n",
    "    subjects_pronouns = ['io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro']\n",
    "\n",
    "    for i,p in zip(subjects_indicies, subjects_pronouns):\n",
    "\n",
    "        dictionary['present'][p] = formatResults(soup.select('table')[0])[i]\n",
    "        dictionary['imperfect'][p] = formatResults(soup.select('table')[2])[i]\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0221f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatImperative(string, infinitive = ''):\n",
    "    tu = re.search(r'\\\\ntu(.*?)\\\\', string.text).group(1)\n",
    "    lui = re.search(r'\\\\nlui(.*?)\\\\', string.text).group(1)\n",
    "    noi = re.search(r'\\\\nnoi(.*?)\\\\', string.text).group(1)\n",
    "    voi = re.search(r'\\\\nvoi(.*?)\\\\', string.text).group(1)\n",
    "    loro = re.search(r'\\\\nloro(.*?)\\\\', string.text).group(1)\n",
    "\n",
    "    tu = re.sub('[\\(].*?[\\)]', '', tu).split(\";\")[0]\n",
    "    lui = re.sub('[\\(].*?[\\)]', '', lui).split(\";\")[0]\n",
    "    noi = re.sub('[\\(].*?[\\)]', '', noi).split(\";\")[0]\n",
    "    voi = re.sub('[\\(].*?[\\)]', '', voi).split(\";\")[0]\n",
    "    loro = re.sub('[\\(].*?[\\)]', '', loro).split(\";\")[0]\n",
    "\n",
    "    if not infinitive:\n",
    "        return [tu, lui, noi, voi, loro]\n",
    "    else:\n",
    "        return [\"non \" + x for x in [infinitive, lui, noi, voi, loro]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SCRAPE REVERSO FOR TRANSLATIONS---#\n",
    "def getReversoTranslations(verb):\n",
    "\n",
    "    translations = []\n",
    "\n",
    "    page = requests.get(f'https://context.reverso.net/translation/italian-english/{verb}', headers = headers)\n",
    "    soup = bs4.BeautifulSoup(page.text)\n",
    "\n",
    "    if verb in soup.select('title')[0].getText():\n",
    "\n",
    "        while len(soup.find_all('div', {\"class\": \"mobile-hidden\"})) > 0:\n",
    "            soup.find_all('div', {\"class\": \"mobile-hidden\"})[0].extract()\n",
    "        \n",
    "        while len(soup.find_all('a', {\"class\": \"mobile-hidden\"})) > 0:  \n",
    "            soup.find_all('a', {\"class\": \"mobile-hidden\"})[0].extract()\n",
    "\n",
    "        for i in range(0, len(soup.select('#translations-content .translation.ltr.dict.v'))):\n",
    "            translations.append(soup.select('#translations-content .translation.ltr.dict.v')[i].getText().replace('\\n\\n\\n\\r\\n          ','').replace('\\n',''))\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---USE GOOGLETRANS PACKAGE TO FIND TRANSLATONS---#\n",
    "def getGoogleTranslations(verb):\n",
    "    \n",
    "    try: \n",
    "        translations = translator.translate(verb, dest = 'en', src = 'it').extra_data['all-translations']\n",
    "\n",
    "        if translations != None:\n",
    "            for x in range(0, len(translations)):\n",
    "                if translations[x][0] == 'verb':\n",
    "                    translations = translations[x][1]\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            translations = []\n",
    "                    \n",
    "    except Exception as ex:\n",
    "            translations = []\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c44345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate(verb_data):\n",
    "    \n",
    "    verb,rank = verb_data\n",
    "\n",
    "    url = f'https://api.verbix.com/conjugator/iv1/ab8e7bb5-9ac6-11e7-ab6a-00089be4dcbc/1/4/104/{verb}'\n",
    "    page = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(page.text, 'lxml') \n",
    "\n",
    "    if rank < 50000:\n",
    "\n",
    "        dictionary = nestedDict()\n",
    "\n",
    "        dictionary['infinitive'] = verb\n",
    "        dictionary['rank'] = rank\n",
    "        dictionary['regularity'] = checkRegularity(verb, page)\n",
    "\n",
    "        dictionary['translations'] = []\n",
    "\n",
    "        dictionary['participle'] = {'present': re.search(r'Participio presente: (.*?)\\\\', soup.select('body')[0].getText()).group(1),\n",
    "                                    'past': re.search(r'Participio passato: (.*?)\\\\', soup.select('body')[0].getText()).group(1)}\n",
    "\n",
    "        gerund = re.search(r'Gerundio: (.*?)\\\\', soup.select('body')[0].getText()).group(1) \n",
    "\n",
    "        subjects_indicies = [0, 1, 2, 2, 3, 4, 5]\n",
    "        subjects_pronouns = ['io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro']\n",
    "\n",
    "        subjects_indicies_imp = [0, 1, 2, 3, 4]\n",
    "        subjects_pronouns_imp = ['tu', 'lei', 'noi', 'voi', 'loro']\n",
    "\n",
    "        for i,p in zip(subjects_indicies, subjects_pronouns):\n",
    "\n",
    "            #indicative\n",
    "            dictionary['simple']['indicative']['present'][p] = formatResults(soup.select('table')[0])[i]\n",
    "            dictionary['simple']['indicative']['preterite'][p] = formatResults(soup.select('table')[6])[i]\n",
    "            dictionary['simple']['indicative']['imperfect'][p] = formatResults(soup.select('table')[2])[i]\n",
    "            dictionary['simple']['indicative']['future'][p] = formatResults(soup.select('table')[4])[i]\n",
    "\n",
    "            #subjunctive\n",
    "            dictionary['simple']['subjunctive']['present'][p] = formatResults(soup.select('table')[8])[i]\n",
    "            dictionary['simple']['subjunctive']['imperfect'][p] = formatResults(soup.select('table')[10])[i]\n",
    "\n",
    "            #conditional\n",
    "            dictionary['simple']['conditional']['conditional'][p] = formatResults(soup.select('table')[12])[i]\n",
    "\n",
    "            #perfect indicative\n",
    "            dictionary['compound']['indicative']['present'][p] = formatResults(soup.select('table')[1])[i]\n",
    "            dictionary['compound']['indicative']['preterite'][p] = formatResults(soup.select('table')[7])[i]\n",
    "            dictionary['compound']['indicative']['imperfect'][p] = formatResults(soup.select('table')[3])[i]\n",
    "            dictionary['compound']['indicative']['future'][p] = formatResults(soup.select('table')[5])[i]\n",
    "\n",
    "            #perfect subjunctive\n",
    "            dictionary['compound']['subjunctive']['present'][p] = formatResults(soup.select('table')[9])[i]\n",
    "            dictionary['compound']['subjunctive']['imperfect'][p] = formatResults(soup.select('table')[11])[i]\n",
    "\n",
    "            #perfect conditional\n",
    "            dictionary['compound']['conditional']['conditional'][p] = formatResults(soup.select('table')[13])[i]\n",
    "\n",
    "            #progressive\n",
    "            dictionary['progressive']['indicative']['present'][p] = f\"{stare['present'][p]} {gerund}\"\n",
    "            dictionary['progressive']['indicative']['imperfect'][p] = f\"{stare['imperfect'][p]} {gerund}\"\n",
    "\n",
    "        #imperative\n",
    "            try:\n",
    "                for i,p in zip(subjects_indicies_imp, subjects_pronouns_imp):\n",
    "                    dictionary['simple']['imperative']['affirmative'][p] = formatImperative(soup.select('table')[14])[i]\n",
    "                    dictionary['simple']['imperative']['negative'][p] = formatImperative(soup.select('table')[14], verb)[i]\n",
    "            except:\n",
    "                for i,p in zip(subjects_indicies_imp, subjects_pronouns_imp):\n",
    "                    dictionary['simple']['imperative']['affirmative'][p] = ''\n",
    "                    dictionary['simple']['imperative']['negative'][p] = ''\n",
    "    \n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SCRAPE UNRANKED VERBS FROM COOLJUGATOR---#\n",
    "page = requests.get('https://cooljugator.com/it/list/all', headers = headers)\n",
    "soup = bs4.BeautifulSoup(page.text, 'lxml')\n",
    "coolverbs = []\n",
    "\n",
    "for item in soup.select('.ui.segment.stacked .item'):\n",
    "    verb = item.getText().split(' ')[0]\n",
    "    if len(item.getText().split(' ')) == 3 and verb.isalpha() and (verb[-2:] == 're' or verb[-2:] == 'si'):\n",
    "        coolverbs.append(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zuppare\n"
     ]
    }
   ],
   "source": [
    "#---SCRAPE UNRANKED VERBS FROM WIKIPEDIA---#\n",
    "wikiverbs = []\n",
    "urlsafe = set([x for x in 'abcdefghijklmnopqrstuvwxyz'])\n",
    "last = 'abalienare'\n",
    "run = True\n",
    "\n",
    "while run:\n",
    "\n",
    "    url = f'https://en.wiktionary.org/w/index.php?title=Category:Italian_verbs&pagefrom={last}'\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = bs4.BeautifulSoup(page.text, 'lxml') \n",
    "    list_items = soup.select('.mw-content-ltr .mw-category li a')\n",
    "\n",
    "    for i in range(18, len(list_items)):\n",
    "        verb = list_items[i].getText()\n",
    "\n",
    "        if verb not in wikiverbs and ' ' not in verb and verb.isalpha() and (verb[-2:] == 're' or verb[-2:] == 'si'):\n",
    "            wikiverbs.append(verb.lower())\n",
    "            clear(); print(wikiverbs[-1])\n",
    "    \n",
    "    for verb in reversed(wikiverbs):\n",
    "        split = set([x for x in verb])  \n",
    "        if len(split - urlsafe) == 0:\n",
    "            if verb != last:\n",
    "                last = verb\n",
    "                break\n",
    "            else:\n",
    "                run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---COMBINE VERBS FROM COOLJUGATOR AND WIKIPEDIA---#\n",
    "all_verbs = list(set(coolverbs + wikiverbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET DEFAULT VARIABLES FOR ADDITIONAL VERBS---#\n",
    "total = all_verbs\n",
    "new_verbs = []\n",
    "initlen = len(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET DEFAULT INDEX FOR ADDITIONAL VERBS---#\n",
    "start = 5102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battersela – 12857 -> 13960 | 1103 (105.849%)\n"
     ]
    }
   ],
   "source": [
    "#---FIND ADDITIONAL VERBS USING VERBIX---#\n",
    "for i in range(start, len(all_verbs)):\n",
    "    \n",
    "    additional = findAdditionalVerbs(all_verbs[i])\n",
    "    \n",
    "    for verb in additional:\n",
    "        if verb not in total:\n",
    "            total.append(verb)\n",
    "            new_verbs.append(verb)\n",
    "    \n",
    "    clear(wait = True); print(f\"{all_verbs[i]} – {initlen} -> {len(total)} | {len(new_verbs)} ({round(i*100/initlen, 3)}%)\")\n",
    "\n",
    "all_verbs = total; del total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET VARIABLES FOR NGRAM SCORES---#\n",
    "verbs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET START INDEX FOR NGRAM SCORING---#\n",
    "start = 12002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codazzo (13959) (99.993%)\n"
     ]
    }
   ],
   "source": [
    "#---ITERATE THROUGH ALL VERBS AND FIND NGRAM SCORES---#\n",
    "for i in range(start, len(all_verbs)):\n",
    "    try:\n",
    "        verb = all_verbs[i]\n",
    "        value = getNgramData(verb, 'italian', (1980, 2005))\n",
    "\n",
    "        if isinstance(value, float):\n",
    "            verbs.append([verb, value])\n",
    "            clear(wait = True); print(f\"{verb} ({i}) ({round(i*100/len(all_verbs), 3)}%)\")\n",
    "        else:\n",
    "            clear(wait = True); print(f\"{verb} ({i}) not found\")\n",
    "        \n",
    "        i += 1; time.sleep(1)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        clear(wait = True); print(f\"Error ({ex}) – Sleeping for 2 minutes ({round(i*100/len(all_verbs), 3)}%)\")\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---RANK VERBS ACCORDING TO NGRAM SCORES---#\n",
    "infinitives = []\n",
    "rank = 1\n",
    "\n",
    "for verb in sorted(verbs, key = lambda x: x[1])[::-1]:\n",
    "    infinitives.append([verb[0], rank])\n",
    "    rank += 1\n",
    "\n",
    "verbs = infinitives; del infinitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11253 -> 11250\n"
     ]
    }
   ],
   "source": [
    "#---FILTER DUPLICATES AND RERANK VERBS---#\n",
    "decoded = []\n",
    "unique = []\n",
    "rank = 1\n",
    "\n",
    "for verb in verbs:\n",
    "    if unidecode(verb[0]) not in decoded:\n",
    "        decoded.append(unidecode(verb[0]))\n",
    "        unique.append([verb[0], rank])\n",
    "        rank += 1\n",
    "\n",
    "print(f\"{len(verbs)} -> {len(unique)}\")\n",
    "\n",
    "infinitives = unique; del unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CREATE INTERMEDIATE CHECKPOINT FILE AS CONTINGENCY IN CASE OF KERNAL TIMEOUT---#\n",
    "f = os.path.join(data_dir, 'infinitives_italian_intermediate.json')\n",
    "\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(infinitives, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = copy.deepcopy(infinitives)\n",
    "\n",
    "for verb in temp:\n",
    "    if (verb[0][-2:] == 're' or verb[0][-2:] == 'si') == False:\n",
    "        infinitives.remove(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET VARIABLES FOR CONJUGATIONS---#\n",
    "stare = getStare()\n",
    "conjugations = defaultdict()\n",
    "defective = set()\n",
    "skipped = []\n",
    "rank = 1\n",
    "exclude = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET INDEX AT WHICH TO START CONJUGATIONS---#\n",
    "start = 1590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soprarrivare – complete (99.991%)\n"
     ]
    }
   ],
   "source": [
    "#---GENERATE CONJUGATIONS---#\n",
    "for i in range(start, len(infinitives)):\n",
    "\n",
    "    infinitive = infinitives[i]\n",
    "    try:\n",
    "        c = conjugate(infinitive)\n",
    "\n",
    "        if c != None and infinitive[0] not in exclude: \n",
    "            conjugations[c['infinitive']] = c\n",
    "            conjugations[infinitive[0]]['rank'] = rank\n",
    "            rank += 1\n",
    "\n",
    "            clear(wait = True), print(f\"{infinitive[0]} – complete ({round(i*100/len(infinitives), 3)}%)\")\n",
    "        \n",
    "        else:\n",
    "            clear(wait = True), print(f\"{infinitive[0]} – skipped ({round(i*100/len(infinitives), 3)}%)\")\n",
    "            skipped.append(infinitive)\n",
    "\n",
    "    except Exception as ex:\n",
    "        clear(wait = True), print(f\"{infinitive[0]} – skipped – {ex} ({round(i*100/len(infinitives), 3)}%)\")\n",
    "        skipped.append(infinitive)\n",
    "\n",
    "conjugations = dict(conjugations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8071 -> 8071\n"
     ]
    }
   ],
   "source": [
    "#---REMOVE VERBS WITH UNKNOWN REGULARITIES---#\n",
    "remove = []\n",
    "rank = 1\n",
    "\n",
    "for verb in conjugations:\n",
    "    if conjugations[verb]['regularity'] == 'x':\n",
    "        remove.append(verb)\n",
    "    else:\n",
    "        conjugations[verb]['rank'] = rank\n",
    "        rank += 1\n",
    "\n",
    "print(f\"{len(conjugations)} -> {len(conjugations) - len(remove)}\")\n",
    "\n",
    "for verb in remove:\n",
    "    conjugations.pop[verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CREATE INTERMEDIATE CHECKPOINT FILE AS CONTINGENCY IN CASE OF KERNAL TIMEOUT---#\n",
    "with open(os.path.join(data_dir, 'conjugations_italian_intermediate.json'), \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(conjugations, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET INDEX VARIABLE TO 1 BY DEFAULT---#\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soprarrivare (8071) – complete (100.0%)\n"
     ]
    }
   ],
   "source": [
    "#---ADD TRANSLATIONS---#\n",
    "for verb in conjugations:\n",
    "\n",
    "    if conjugations[verb]['rank'] > i - 1 and conjugations[verb]['translations'] == []:\n",
    "        \n",
    "        translations = getReversoTranslations(verb)\n",
    "\n",
    "        if not translations:\n",
    "            translations = getGoogleTranslations(verb)\n",
    "            time.sleep(9)\n",
    "\n",
    "        conjugations[verb]['translations'] = translations\n",
    "\n",
    "        if conjugations[verb]['rank'] % 200 == 0:\n",
    "            with open(os.path.join(data_dir, 'conjugations_translated_italian_intermediate.json'), \"w\", encoding = 'utf8') as file:\n",
    "                json.dump(conjugations,file, indent = 4, ensure_ascii = False)\n",
    "\n",
    "        clear(wait = True), print(f\"{verb} ({i}) – complete ({round(i*100/len(conjugations), 3)}%)\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---REMOVE VERBS WITH NO TRANSLATIONS---#\n",
    "remove = []\n",
    "rank = 1\n",
    "\n",
    "for verb in conjugations:\n",
    "    if conjugations[verb]['translations'] == []:\n",
    "        remove.append(verb)\n",
    "    else:\n",
    "        conjugations[verb]['rank'] = rank\n",
    "        rank += 1\n",
    "\n",
    "for verb in remove:\n",
    "    conjugations.pop(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SAVE CONJUGATIONS TO JSON---#\n",
    "f = os.path.join(data_dir, 'conjugations_italian.json')\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(conjugations, file, indent = 4, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---PARSE INFINITIVES AND SAVE TO JSON---#\n",
    "infinitives = []\n",
    "\n",
    "for verb in conjugations:\n",
    "    infinitives.append([verb, conjugations[verb]['rank'], conjugations[verb]['regularity']])\n",
    "\n",
    "f = os.path.join(data_dir, 'infinitives_italian.json')\n",
    "with open(f, \"w\", encoding = 'utf8') as file:\n",
    "    json.dump(infinitives, file, indent = 4, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9596ba951ddb21d01b6d37648fb60066e40c7f4a0a12dddda8d43c6adacb1362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
